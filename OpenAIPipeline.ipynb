{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization"
      ],
      "metadata": {
        "id": "rucOACZvCG2l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azsPeLcvJjbr"
      },
      "outputs": [],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Bio"
      ],
      "metadata": {
        "id": "1RT9NhtzLZF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "def get_biopython_abstracts_by_pmids(pmids):\n",
        "  Entrez.email = \"zhalayev@gmail.com\"\n",
        "  handle = Entrez.efetch(db=\"pubmed\", id=pmids, rettype=\"xml\")\n",
        "  records = Entrez.read(handle)\n",
        "  biomedexplorer_abstracts_with_pmids = []\n",
        "  for record in records['PubmedArticle']:\n",
        "    article = record['MedlineCitation']['Article']\n",
        "    pmid = record['MedlineCitation']['PMID']\n",
        "    abstract_elements = article.get('Abstract', {}).get('AbstractText', '')\n",
        "    # Convert StringElement to string and filter out empty strings\n",
        "    abstracts = [str(abstract_element) for abstract_element in abstract_elements if str(abstract_element)]\n",
        "    biomedexplorer_abstracts_with_pmids.append({\"PMID\": str(pmid), \"Abstract\": abstracts})\n",
        "  for entry in biomedexplorer_abstracts_with_pmids:\n",
        "    if entry['Abstract'] == []:\n",
        "      entry['Abstract'] = None\n",
        "    else:\n",
        "      entry['Abstract'] = ' '.join(entry['Abstract'])\n",
        "  return biomedexplorer_abstracts_with_pmids"
      ],
      "metadata": {
        "id": "QZ8tuoLTLZDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'), organization=\"org-cWmpmr0PaRcJuukG9B1grwHm\")"
      ],
      "metadata": {
        "id": "U6N5i9v-K0Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MDURnMAELqNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_abstracts_df(abstracts):\n",
        "  abstracts_df = pd.DataFrame(abstracts, columns=['PMID', 'Abstract'])\n",
        "  abstracts_df = abstracts_df.dropna(subset=['PMID', 'Abstract'])\n",
        "  return abstracts_df"
      ],
      "metadata": {
        "id": "tQDMMfAxLSb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_abstracts_str(abstracts_df):\n",
        "  abstracts_lst = abstracts_df['Abstract'].tolist()\n",
        "  abstracts_str = \"   \".join(abstracts_lst)\n",
        "  abstracts_str = abstracts_str[:64000]\n",
        "  return abstracts_str"
      ],
      "metadata": {
        "id": "e7JEeiMULseT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def openai_summarize(abstracts_str):\n",
        "  openai_response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0125\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Provide clear and concise summary of the abstracts retrieved from PubMed in one small paragraph\"},\n",
        "      {\"role\": \"user\", \"content\": abstracts_str}\n",
        "    ]\n",
        "  )\n",
        "  return openai_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "dlqjjCUzK4aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_df(csv):\n",
        "  df = pd.read_csv(csv)\n",
        "  df = df.drop(columns=['Unnamed: 0'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "hLuGohjKNNtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BioASQ_all_summary_df = csv_to_df('bioasq_summaries.csv')\n",
        "BioASQ_all_summary_df"
      ],
      "metadata": {
        "id": "gr0y-vgoNOmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install metapub"
      ],
      "metadata": {
        "id": "4AmFHwMjN3X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export NCBI_API_KEY=userdata.get('PUBMED_API_KEY')"
      ],
      "metadata": {
        "id": "XaMQNUnYN1Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from metapub import PubMedFetcher\n",
        "\n",
        "num_of_articles=20"
      ],
      "metadata": {
        "id": "tzIYBs-_OB7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_pmids_by_query(keyword, num_of_articles):\n",
        "  fetch = PubMedFetcher()\n",
        "  return fetch.pmids_for_query(keyword, retmax=num_of_articles)"
      ],
      "metadata": {
        "id": "nhSQjvt4NufK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_operation(row):\n",
        "  pmids = fetch_pmids_by_query(row['question'], num_of_articles)\n",
        "  if len(pmids) == 0:\n",
        "    pmids = row['pmids']\n",
        "  abstracts = get_biopython_abstracts_by_pmids(pmids)\n",
        "  print(\"Question: \\n\" + row[\"question\"] + \"\\n\")\n",
        "  print(abstracts)\n",
        "  abstracts_df = get_abstracts_df(abstracts)\n",
        "  abstracts_str = get_abstracts_str(abstracts_df)\n",
        "  return openai_summarize(abstracts_str)"
      ],
      "metadata": {
        "id": "nG_qEpE3NTqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_chunk(chunk, csv_path, checkpoint_file):\n",
        "    # Process chunk and write to CSV\n",
        "    chunk_results = []\n",
        "    for index, row in chunk.iterrows():\n",
        "        try:\n",
        "            print(\"-------------\")\n",
        "            result = combined_operation(row)\n",
        "            print(\"-------------\")\n",
        "            question = row['question']\n",
        "            chunk_results.append({'question': question, 'OpenAI PubMed Abstracts Summary': result})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row at index {index}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame and append to CSV\n",
        "    chunk_df = pd.DataFrame(chunk_results)\n",
        "    chunk_df.to_csv(csv_path, mode='a', header=False, index=False)"
      ],
      "metadata": {
        "id": "uGnwrTuKONF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def main(df, csv_path):\n",
        "    # Define the size of each chunk\n",
        "    chunk_size = 60\n",
        "\n",
        "    # Create a CSV file and write the header\n",
        "    pd.DataFrame(columns=['question','OpenAI Pubmed Abstracts Summary']).to_csv(csv_path, index=False)\n",
        "\n",
        "    # Start from the beginning or the last checkpoint\n",
        "    start_index = 0\n",
        "    checkpoint_file = 'checkpoint.txt'\n",
        "    try:\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            start_index = int(f.read().strip()) + 1\n",
        "    except FileNotFoundError:\n",
        "        start_index = 0\n",
        "\n",
        "    # Chunk the DataFrame\n",
        "    for start in range(start_index, len(df), chunk_size):\n",
        "        end = min(start + chunk_size, len(df))\n",
        "        chunk = df.iloc[start:end]\n",
        "        try:\n",
        "            process_chunk(chunk, csv_path, checkpoint_file)\n",
        "            # Save the last index of the chunk to checkpoint file\n",
        "            with open(checkpoint_file, 'w') as f:\n",
        "                f.write(str(end - 1))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk starting at index {start}: {e}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "MW4H3tYROWCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = BioASQ_all_summary_df.head(1)\n",
        "df"
      ],
      "metadata": {
        "id": "qwDv-Kb0OgF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(BioASQ_all_summary_df, 'results.csv')"
      ],
      "metadata": {
        "id": "mJqjY2jDOalg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "LJb6NtDP_v_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.read_csv(\"results.csv\")\n",
        "results_df"
      ],
      "metadata": {
        "id": "53TghuI1QDX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ItTWXetbrI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simplification"
      ],
      "metadata": {
        "id": "U_sYhP-rCKkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "adaptations = []\n",
        "abstracts = []\n",
        "pmids = []\n",
        "question = []\n",
        "adaptation_version = []\n",
        "question_type = []\n",
        "# Download dataset, create connected strings (with '  ' replaced by ' ') and append to lists\n",
        "data = json.load(open('data.json', 'r'))\n",
        "\n",
        "# Work through every question number\n",
        "for question_number, value in data.items():\n",
        "\n",
        "    # Work through every PMID\n",
        "    for pmid, texts in value.items():\n",
        "        if (pmid != 'question') and (pmid != 'question_type'):\n",
        "\n",
        "            # Append abstracts and adaptations\n",
        "            # If there are multiple adaptations, duplicate the abstract, pmid, and question\n",
        "            for i in range(sum('adaptation' in key for key in texts['adaptations'].keys())):\n",
        "\n",
        "                pmids.append(pmid)\n",
        "                question.append(question_number)\n",
        "                abstracts.append(' '.join(texts['abstract'].values()))\n",
        "                question_type.append(data[question_number]['question_type'])\n",
        "\n",
        "            if 'adaptation1' in texts['adaptations'].keys():\n",
        "                adaptations.append(' '.join(texts['adaptations']['adaptation1'].values()).replace('  ', ' '))\n",
        "                adaptation_version.append(1)\n",
        "\n",
        "\n",
        "            if 'adaptation2' in texts['adaptations'].keys():\n",
        "                adaptations.append(' '.join(texts['adaptations']['adaptation2'].values()).replace('  ', ' '))\n",
        "                adaptation_version.append(2)\n",
        "\n",
        "\n",
        "            if 'adaptation3' in texts['adaptations'].keys():\n",
        "                adaptations.append(' '.join(texts['adaptations']['adaptation3'].values()).replace('  ', ' '))\n",
        "                adaptation_version.append(2)\n",
        "\n",
        "\n",
        "dataset = pd.DataFrame({'question':question, 'pmid':pmids, 'input_text':abstracts, 'target_text':adaptations,\n",
        "                        'Adaptation_Version': adaptation_version, 'Question_Type': question_type})\n",
        "dataset"
      ],
      "metadata": {
        "id": "zU3BW8tFDolE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "a2X7gKCqCQ2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'), organization=\"org-cWmpmr0PaRcJuukG9B1grwHm\")"
      ],
      "metadata": {
        "id": "PJCHR1R0CcDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def openai_simplify(row):\n",
        "  print(\"Input: \" + row['input_text'])\n",
        "  openai_response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-0125\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"Provide clear and concise simplified summary of the abstracts retrieved from PubMed in one small paragraph\"},\n",
        "      {\"role\": \"user\", \"content\": row['input_text']}\n",
        "    ]\n",
        "  )\n",
        "  return openai_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "NXkHIspCCfsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_simplification_chunk(chunk, csv_path, checkpoint_file):\n",
        "    # Process chunk and write to CSV\n",
        "    chunk_results = []\n",
        "    for index, row in chunk.iterrows():\n",
        "        try:\n",
        "            print(\"-------------\")\n",
        "            print(\"Index: \" + str(index))\n",
        "            result = openai_simplify(row)\n",
        "            print(\"Result: \" + result)\n",
        "            print(\"-------------\")\n",
        "            pmid = row['pmid']\n",
        "            golden_standard = row['target_text']\n",
        "            chunk_results.append({'pmid': pmid, 'OpenAI PLABA Abstracts Simplification': result, \"Golden standard\" : golden_standard})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row at index {index}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Convert to DataFrame and append to CSV\n",
        "    chunk_df = pd.DataFrame(chunk_results)\n",
        "    chunk_df.to_csv(csv_path, mode='a', header=False, index=False)"
      ],
      "metadata": {
        "id": "JVmNKzG1DEa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def main_simplification(df, csv_path):\n",
        "    # Define the size of each chunk\n",
        "    chunk_size = 60\n",
        "\n",
        "    # Create a CSV file and write the header\n",
        "    pd.DataFrame(columns=['pmid','OpenAI PLABA Abstracts Simplification', 'Golden standard']).to_csv(csv_path, index=False)\n",
        "\n",
        "    # Start from the beginning or the last checkpoint\n",
        "    start_index = 0\n",
        "    checkpoint_file = 'checkpoint.txt'\n",
        "    try:\n",
        "        with open(checkpoint_file, 'r') as f:\n",
        "            start_index = int(f.read().strip()) + 1\n",
        "    except FileNotFoundError:\n",
        "        start_index = 0\n",
        "\n",
        "    # Chunk the DataFrame\n",
        "    for start in range(start_index, len(df), chunk_size):\n",
        "        end = min(start + chunk_size, len(df))\n",
        "        chunk = df.iloc[start:end]\n",
        "        try:\n",
        "            process_simplification_chunk(chunk, csv_path, checkpoint_file)\n",
        "            # Save the last index of the chunk to checkpoint file\n",
        "            with open(checkpoint_file, 'w') as f:\n",
        "                f.write(str(end - 1))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk starting at index {start}: {e}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "nWGgy80IDQfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_simplification(dataset.head(5), 'openai_simplification_results.csv')"
      ],
      "metadata": {
        "id": "GvadTHPHDT6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dsf = pd.read_csv('openai_simplification_results.csv')\n",
        "dsf"
      ],
      "metadata": {
        "id": "b4817j76EqI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_simplification(dataset, 'openai_simplification_results.csv')"
      ],
      "metadata": {
        "id": "28aVNgS-F658"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.read_csv('openai_simplification_results.csv')\n",
        "results_df"
      ],
      "metadata": {
        "id": "srBbpiITF-HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Summaries"
      ],
      "metadata": {
        "id": "6Wdt8Uh4Mh13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "lakjAo52MsjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_df(csv):\n",
        "  df = pd.read_csv(csv)\n",
        "  df = df.drop(columns=['Unnamed: 0'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "-HU1p2aWM5cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_summaries_ideal_df = csv_to_df('q_summaries_ideal.csv')\n",
        "q_summaries_ideal_df"
      ],
      "metadata": {
        "id": "e6m9DsmmMkAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge --quiet\n",
        "!pip install bert_score --quiet"
      ],
      "metadata": {
        "id": "CF-K8hpTM-q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import BERTScorer\n",
        "\n",
        "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
      ],
      "metadata": {
        "id": "8V9bujmKM1XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def highlight_max(s):\n",
        "    is_max = s == s.max()\n",
        "    return [\"background-color: green\" if v else \"background-color: black\" for v in is_max]"
      ],
      "metadata": {
        "id": "YcqjrNiMNSPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_bert_scores(df):\n",
        "  bert_scores_list = []\n",
        "\n",
        "  for idx, row in df.iterrows():\n",
        "      print(\"Index: \" + str(idx))\n",
        "      print(\"Question: \" + row['question'])\n",
        "      P1, R1, F1 = scorer.score([row['OpenAI Pubmed Abstracts Summary']], [row['ideal_answer']])\n",
        "      P2, R2, F2 = scorer.score([row[\"Gemini PubMed Abstracts Summary\"]], [row['ideal_answer']])\n",
        "      row = {\n",
        "          \"Metric\": \"F1 score\",\n",
        "          \"OpenAI Pubmed Abstracts Summary\": F1.tolist()[0],\n",
        "          \"Gemini PubMed Abstracts Summary\": F2.tolist()[0],\n",
        "          \"Index\": idx\n",
        "      }\n",
        "      bert_scores_list.append(row)\n",
        "  return bert_scores_list"
      ],
      "metadata": {
        "id": "M0lh-nYFNFmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_scores_list = eval_bert_scores(q_summaries_ideal_df)"
      ],
      "metadata": {
        "id": "xEFiIV9zNLEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_scores_df = pd.DataFrame(bert_scores_list).set_index([\"Index\", \"Metric\"])\n",
        "bert_scores_styled = bert_scores_df.style.apply(highlight_max, axis=1)\n",
        "bert_scores_styled"
      ],
      "metadata": {
        "id": "nF1-TiwNNNE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_scores_df.to_csv('bert_scores_df.csv')"
      ],
      "metadata": {
        "id": "9nqnZ4MMOsvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of Simplification"
      ],
      "metadata": {
        "id": "RyThiIrGPtkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_simpl_df = pd.read_csv('openai_simplification_results.csv')\n",
        "openai_simpl_df"
      ],
      "metadata": {
        "id": "W4TCkbjbPvlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_simpl_df = pd.read_csv('simplification_result.csv')\n",
        "gemini_simpl_df"
      ],
      "metadata": {
        "id": "3z0lIvb4P_5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "adaptations = []\n",
        "abstracts = []\n",
        "pmids = []\n",
        "question = []\n",
        "adaptation_version = []\n",
        "question_type = []\n",
        "# Download dataset, create connected strings (with '  ' replaced by ' ') and append to lists\n",
        "data = json.load(open('data.json', 'r'))\n",
        "\n",
        "# Work through every question number\n",
        "for question_number, value in data.items():\n",
        "\n",
        "    # Work through every PMID\n",
        "    for pmid, texts in value.items():\n",
        "        if (pmid != 'question') and (pmid != 'question_type'):\n",
        "\n",
        "            # Append abstracts and adaptations\n",
        "            # If there are multiple adaptations, duplicate the abstract, pmid, and question\n",
        "            for i in range(sum('adaptation' in key for key in texts['adaptations'].keys())):\n",
        "\n",
        "                pmids.append(pmid)\n",
        "                question.append(question_number)\n",
        "                abstracts.append(' '.join(texts['abstract'].values()))\n",
        "                question_type.append(data[question_number]['question_type'])\n",
        "\n",
        "            if 'adaptation1' in texts['adaptations'].keys():\n",
        "                adaptations.append(' '.join(texts['adaptations']['adaptation1'].values()).replace('  ', ' '))\n",
        "                adaptation_version.append(1)\n",
        "\n",
        "\n",
        "            if 'adaptation2' in texts['adaptations'].keys():\n",
        "                adaptations.append(' '.join(texts['adaptations']['adaptation2'].values()).replace('  ', ' '))\n",
        "                adaptation_version.append(2)\n",
        "\n",
        "\n",
        "            if 'adaptation3' in texts['adaptations'].keys():\n",
        "                adaptations.append(' '.join(texts['adaptations']['adaptation3'].values()).replace('  ', ' '))\n",
        "                adaptation_version.append(2)\n",
        "\n",
        "\n",
        "dataset = pd.DataFrame({'question':question, 'pmid':pmids, 'input_text':abstracts, 'target_text':adaptations,\n",
        "                        'Adaptation_Version': adaptation_version, 'Question_Type': question_type})\n",
        "dataset"
      ],
      "metadata": {
        "id": "IifcFVgHTAgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.loc[:, ['pmid', 'input_text']]\n",
        "dataset"
      ],
      "metadata": {
        "id": "NjknjnB9UV82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_simplification_df = pd.merge(openai_simpl_df, gemini_simpl_df, on='pmid')\n",
        "all_simplification_df"
      ],
      "metadata": {
        "id": "M9n4jlhgQR_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_simplification_df = pd.merge(all_simplification_df, dataset, on='pmid')\n",
        "all_simplification_df"
      ],
      "metadata": {
        "id": "QpT6iZKCUrsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_simplification_df = all_simplification_df.drop_duplicates(subset=['pmid'])\n",
        "all_simplification_df"
      ],
      "metadata": {
        "id": "DvjHckG5RI2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_simplification_df.to_csv('all_simplification_results.csv')"
      ],
      "metadata": {
        "id": "0YMhSWfBRivu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_simplification_df = csv_to_df('all_simplification_results.csv')\n",
        "all_simplification_df"
      ],
      "metadata": {
        "id": "o2B_j_vIcmRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simplification/modified_sari.py implementation"
      ],
      "metadata": {
        "id": "diXeeUKtWhRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from collections import Counter\n",
        "import glob\n",
        "import numpy as np\n",
        "from argparse import ArgumentParser"
      ],
      "metadata": {
        "id": "NiUfSQOvR0H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_subsequence(str1,str2):\n",
        "    m = len(str1)\n",
        "    n = len(str2)\n",
        "    i, j = 0, 0\n",
        "    while j<m and i<n:\n",
        "        if str1[j] == str2[i]:\n",
        "            j = j+1\n",
        "        i = i + 1\n",
        "    return j==m\n",
        "\n",
        "def SARIngram(sgrams, cgrams, rgramslist, numref, complex):\n",
        "    rgramsall = [rgram for rgrams in rgramslist for rgram in rgrams]\n",
        "    rgramcounter = Counter(rgramsall)\n",
        "\n",
        "    sgramcounter = Counter(sgrams)\n",
        "    sgramcounter_rep = Counter()\n",
        "    for sgram, scount in sgramcounter.items():\n",
        "        sgramcounter_rep[sgram] = scount * numref\n",
        "\n",
        "    cgramcounter = Counter(cgrams)\n",
        "    cgramcounter_rep = Counter()\n",
        "    for cgram, ccount in cgramcounter.items():\n",
        "        cgramcounter_rep[cgram] = ccount * numref\n",
        "\n",
        "    # KEEP\n",
        "    keepgramcounter_rep = sgramcounter_rep & cgramcounter_rep\n",
        "    keepgramcountergood_rep = keepgramcounter_rep & rgramcounter\n",
        "    keepgramcounterall_rep = sgramcounter_rep & rgramcounter\n",
        "\n",
        "    keeptmpscore1 = 0\n",
        "    keeptmpscore2 = 0\n",
        "    for keepgram in keepgramcountergood_rep:\n",
        "        keeptmpscore1 += keepgramcountergood_rep[keepgram] / keepgramcounter_rep[keepgram]\n",
        "        keeptmpscore2 += keepgramcountergood_rep[keepgram] / keepgramcounterall_rep[keepgram]\n",
        "        # print \"KEEP\", keepgram, keepscore, cgramcounter[keepgram], sgramcounter[keepgram], rgramcounter[keepgram]\n",
        "    keepscore_precision = 0\n",
        "    if len(keepgramcounter_rep) > 0:\n",
        "        keepscore_precision = keeptmpscore1 / len(keepgramcounter_rep)\n",
        "    keepscore_recall = 0\n",
        "    if len(keepgramcounterall_rep) > 0:\n",
        "        keepscore_recall = keeptmpscore2 / len(keepgramcounterall_rep)\n",
        "    keepscore = 0\n",
        "    if keepscore_precision > 0 or keepscore_recall > 0:\n",
        "        keepscore = 2 * keepscore_precision * keepscore_recall / (keepscore_precision + keepscore_recall)\n",
        "\n",
        "    # DELETION\n",
        "    delgramcounter_rep = sgramcounter_rep - cgramcounter_rep\n",
        "    delgramcountergood_rep = delgramcounter_rep - rgramcounter\n",
        "    delgramcounterall_rep = sgramcounter_rep - rgramcounter\n",
        "    deltmpscore1 = 0\n",
        "    deltmpscore2 = 0\n",
        "    for delgram in delgramcountergood_rep:\n",
        "        deltmpscore1 += delgramcountergood_rep[delgram] / delgramcounter_rep[delgram]\n",
        "        deltmpscore2 += delgramcountergood_rep[delgram] / delgramcounterall_rep[delgram]\n",
        "    delscore_precision = 0\n",
        "    if len(delgramcounter_rep) > 0:\n",
        "        delscore_precision = deltmpscore1 / len(delgramcounter_rep)\n",
        "    delscore_recall = 0\n",
        "    if len(delgramcounterall_rep) > 0:\n",
        "        delscore_recall = deltmpscore1 / len(delgramcounterall_rep)\n",
        "    delscore = 0\n",
        "    if delscore_precision > 0 or delscore_recall > 0:\n",
        "        delscore = 2 * delscore_precision * delscore_recall / (delscore_precision + delscore_recall)\n",
        "\n",
        "    # ADDITION\n",
        "    addgramcounter = set(cgramcounter) - set(sgramcounter)\n",
        "    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n",
        "    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n",
        "\n",
        "    sgrams_set = set()\n",
        "    for gram in sgrams:\n",
        "        sgrams_set.update(gram.split())\n",
        "\n",
        "    addgramcountergood_new = set()\n",
        "    for gram in addgramcountergood:\n",
        "        if any([tok not in sgrams_set for tok in gram.split()]) or not is_subsequence(gram.split(), complex.split()):\n",
        "            addgramcountergood_new.add(gram)\n",
        "    addgramcountergood = addgramcountergood_new\n",
        "\n",
        "    addtmpscore = 0\n",
        "    for _ in addgramcountergood:\n",
        "        addtmpscore += 1\n",
        "\n",
        "    addscore_precision = 0\n",
        "    addscore_recall = 0\n",
        "    if len(addgramcounter) > 0:\n",
        "        addscore_precision = addtmpscore / len(addgramcounter)\n",
        "    if len(addgramcounterall) > 0:\n",
        "        addscore_recall = addtmpscore / len(addgramcounterall)\n",
        "    addscore = 0\n",
        "    if addscore_precision > 0 or addscore_recall > 0:\n",
        "        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n",
        "\n",
        "    return (keepscore, (delscore_precision, delscore_recall, delscore), addscore)\n",
        "\n",
        "\n",
        "def SARIsent(ssent, csent, rsents):\n",
        "    numref = len(rsents)\n",
        "\n",
        "    s1grams = ssent.lower().split(\" \")\n",
        "    c1grams = csent.lower().split(\" \")\n",
        "    s2grams = []\n",
        "    c2grams = []\n",
        "    s3grams = []\n",
        "    c3grams = []\n",
        "    s4grams = []\n",
        "    c4grams = []\n",
        "\n",
        "    r1gramslist = []\n",
        "    r2gramslist = []\n",
        "    r3gramslist = []\n",
        "    r4gramslist = []\n",
        "\n",
        "    for rsent in rsents:\n",
        "        r1grams = rsent.lower().split(\" \")\n",
        "        r2grams = []\n",
        "        r3grams = []\n",
        "        r4grams = []\n",
        "        r1gramslist.append(r1grams)\n",
        "        for i in range(0, len(r1grams) - 1):\n",
        "            if i < len(r1grams) - 1:\n",
        "                r2gram = r1grams[i] + \" \" + r1grams[i + 1]\n",
        "                r2grams.append(r2gram)\n",
        "            if i < len(r1grams) - 2:\n",
        "                r3gram = r1grams[i] + \" \" + r1grams[i + 1] + \" \" + r1grams[i + 2]\n",
        "                r3grams.append(r3gram)\n",
        "            if i < len(r1grams) - 3:\n",
        "                r4gram = r1grams[i] + \" \" + r1grams[i + 1] + \" \" + r1grams[i + 2] + \" \" + r1grams[i + 3]\n",
        "                r4grams.append(r4gram)\n",
        "        r2gramslist.append(r2grams)\n",
        "        r3gramslist.append(r3grams)\n",
        "        r4gramslist.append(r4grams)\n",
        "\n",
        "    for i in range(0, len(s1grams) - 1):\n",
        "        if i < len(s1grams) - 1:\n",
        "            s2gram = s1grams[i] + \" \" + s1grams[i + 1]\n",
        "            s2grams.append(s2gram)\n",
        "        if i < len(s1grams) - 2:\n",
        "            s3gram = s1grams[i] + \" \" + s1grams[i + 1] + \" \" + s1grams[i + 2]\n",
        "            s3grams.append(s3gram)\n",
        "        if i < len(s1grams) - 3:\n",
        "            s4gram = s1grams[i] + \" \" + s1grams[i + 1] + \" \" + s1grams[i + 2] + \" \" + s1grams[i + 3]\n",
        "            s4grams.append(s4gram)\n",
        "\n",
        "    for i in range(0, len(c1grams) - 1):\n",
        "        if i < len(c1grams) - 1:\n",
        "            c2gram = c1grams[i] + \" \" + c1grams[i + 1]\n",
        "            c2grams.append(c2gram)\n",
        "        if i < len(c1grams) - 2:\n",
        "            c3gram = c1grams[i] + \" \" + c1grams[i + 1] + \" \" + c1grams[i + 2]\n",
        "            c3grams.append(c3gram)\n",
        "        if i < len(c1grams) - 3:\n",
        "            c4gram = c1grams[i] + \" \" + c1grams[i + 1] + \" \" + c1grams[i + 2] + \" \" + c1grams[i + 3]\n",
        "            c4grams.append(c4gram)\n",
        "\n",
        "    (keep1score, del1score, add1score) = SARIngram(s1grams, c1grams, r1gramslist, numref, ssent)\n",
        "    (keep2score, del2score, add2score) = SARIngram(s2grams, c2grams, r2gramslist, numref, ssent)\n",
        "    (keep3score, del3score, add3score) = SARIngram(s3grams, c3grams, r3gramslist, numref, ssent)\n",
        "    (keep4score, del4score, add4score) = SARIngram(s4grams, c4grams, r4gramslist, numref, ssent)\n",
        "\n",
        "    del1p, del1r, del1f = del1score\n",
        "    del2p, del2r, del2f = del2score\n",
        "    del3p, del3r, del3f = del3score\n",
        "    del4p, del4r, del4f = del4score\n",
        "\n",
        "    avgkeepscore = sum([keep1score, keep2score, keep3score, keep4score]) / 4\n",
        "    avgdelpscore = sum([del1p, del2p, del3p, del4p]) / 4\n",
        "    avgdelrscore = sum([del1r, del2r, del3r, del4r]) / 4\n",
        "    avgdelfscore = sum([del1f, del2f, del3f, del4f]) / 4\n",
        "    avgaddscore = sum([add1score, add2score, add3score, add4score]) / 4\n",
        "    finalpscore = (avgkeepscore + avgdelpscore + avgaddscore) / 3\n",
        "    finalfscore = (avgkeepscore + avgdelfscore + avgaddscore) / 3\n",
        "    return avgkeepscore, (avgdelpscore, avgdelrscore, avgdelfscore), avgaddscore, (finalpscore, finalfscore)\n",
        "\n",
        "\n",
        "def compute_sari(complex_sentences, reference_sentences, simplified_sentences):\n",
        "\n",
        "    delp_scores = list()\n",
        "    delr_scores = list()\n",
        "    delf_scores = list()\n",
        "    add_scores = list()\n",
        "    sari_scores = list()\n",
        "    sarif_scores = list()\n",
        "    keep_scores = list()\n",
        "    for i in range(len(simplified_sentences)):\n",
        "        keep, dels, add, final = SARIsent(complex_sentences[i], simplified_sentences[i],\n",
        "                                          reference_sentences[i])\n",
        "        add_scores.append(add)\n",
        "        delp_scores.append(dels[0])\n",
        "        delr_scores.append(dels[1])\n",
        "        delf_scores.append(dels[2])\n",
        "        keep_scores.append(keep)\n",
        "        sari_scores.append(final[0])\n",
        "        sarif_scores.append(final[1])\n",
        "\n",
        "    return np.mean(sari_scores), np.mean(sarif_scores), np.mean(add_scores), np.mean(keep_scores), np.mean(\n",
        "        delp_scores), np.mean(delr_scores), np.mean(delf_scores)"
      ],
      "metadata": {
        "id": "kCmFj8AXWoLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sari_for_df(df):\n",
        "    sari_scores_openai  = []\n",
        "    sari_scores_gemini = []\n",
        "\n",
        "    # Loop over the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        # Extract the sentences for SARI calculation\n",
        "        complex_sentence = [row['input_text']]  # it expects a list of sentences, hence the brackets\n",
        "        reference_sentences = [[row['Golden standard']]]  # list of lists, each inner list corresponds to one reference\n",
        "        # simplified_sentence = row['OpenAI PLABA Abstracts Simplification']  # assuming you want to calculate for OpenAI's simplifications\n",
        "\n",
        "        simplified_sentence_openai = row['OpenAI PLABA Abstracts Simplification']\n",
        "        # Gemini simplification\n",
        "        simplified_sentence_gemini = row[\"Gemini PLABA Abstracts Simplification\"]\n",
        "        # Compute SARI scores\n",
        "        avg_sari_score_openai, avg_sarif_score_openai, avg_add_score_openai, avg_keep_score_openai, avg_delp_score_openai, avg_delr_score_openai, avg_delf_score_openai = compute_sari(\n",
        "            complex_sentence,\n",
        "            reference_sentences,\n",
        "            [simplified_sentence_openai]  # it expects a list of simplified sentences\n",
        "        )\n",
        "\n",
        "        # Append the scores to the lists\n",
        "        sari_scores_openai.append(avg_sari_score_openai)\n",
        "\n",
        "        avg_sari_score_gemini, avg_sarif_score_gemini, avg_add_score_gemini, avg_keep_score_gemini, avg_delp_score_gemini, avg_delr_score_gemini, avg_delf_score_gemini = compute_sari(\n",
        "            complex_sentence,\n",
        "            reference_sentences,\n",
        "            [simplified_sentence_gemini]  # it expects a list of simplified sentences\n",
        "        )\n",
        "\n",
        "        # Append the scores to the lists\n",
        "        sari_scores_gemini.append(avg_sari_score_gemini)\n",
        "\n",
        "    # Add the scores back into the DataFrame\n",
        "    df['OpenAI SARI Score'] = sari_scores_openai\n",
        "    df['Gemini SARI Score'] = sari_scores_gemini\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Mg2x3odjWt4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_simplification_df = compute_sari_for_df(all_simplification_df)\n",
        "all_simplification_df"
      ],
      "metadata": {
        "id": "iLgjEqJIX_6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_simplification_df.to_csv('SARI_scores_results.csv')"
      ],
      "metadata": {
        "id": "6b4eWAtOgyU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean SARI score for OpenAI\n",
        "openai_mean_sari = all_simplification_df['OpenAI SARI Score'].mean()\n",
        "\n",
        "# Calculate the mean SARI score for Gemini\n",
        "gemini_mean_sari = all_simplification_df['Gemini SARI Score'].mean()\n",
        "\n",
        "# Determine which is higher and output the result\n",
        "if openai_mean_sari > gemini_mean_sari:\n",
        "    print(f\"OpenAI scored higher on average with a SARI score of {openai_mean_sari:.3f}\")\n",
        "elif gemini_mean_sari > openai_mean_sari:\n",
        "    print(f\"Gemini scored higher on average with a SARI score of {gemini_mean_sari:.3f}\")\n",
        "else:\n",
        "    print(f\"Both systems scored the same on average with a SARI score of {openai_mean_sari:.3f}\")\n"
      ],
      "metadata": {
        "id": "e1HHGzj0dUVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_mean_sari"
      ],
      "metadata": {
        "id": "MPxOEzzHfjmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_mean_sari"
      ],
      "metadata": {
        "id": "3u8vVGsYfk-s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}